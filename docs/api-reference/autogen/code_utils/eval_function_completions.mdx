---
sidebarTitle: eval_function_completions
title: autogen.code_utils.eval_function_completions
---

<code class="doc-symbol doc-symbol-heading doc-symbol-function"></code>
#### eval_function_completions

```python
eval_function_completions(
    responses: list[str],
    definition: str,
    test: str | None = None,
    entry_point: str | None = None,
    assertions: str | Callable[[str], tuple[str, float]] | None = None,
    timeout: float | None = 3,
    use_docker: bool | None = True
) -> dict
```

    `(openai<1)` Select a response from a list of responses for the function completion task (using generated assertions), and/or evaluate if the task is successful using a gold test.<br/>

<b>Parameters:</b>
| Name | Description |
|--|--|
| `responses` | The list of responses.<br/><br/>**Type:** list[str] |
| `definition` | The input definition.<br/><br/>**Type:** str |
| `test` | The test code.<br/><br/>**Type:** str \| None<br/><br/>**Default:** None |
| `entry_point` | The name of the function.<br/><br/>**Type:** str \| None<br/><br/>**Default:** None |
| `assertions` | The assertion code which serves as a filter of the responses, or an assertion generator.<br/><br/>When provided, only the responses that pass the assertions will be considered for the actual test (if provided).<br/><br/>**Type:** str \| Callable[[str], tuple[str, float]] \| None<br/><br/>**Default:** None |
| `timeout` | The timeout for executing the code.<br/><br/>**Type:** float \| None<br/><br/>**Default:** 3 |
| `use_docker` | **Type:** bool \| None<br/><br/>**Default:** True |

<b>Returns:</b>
| Type | Description |
|--|--|
| dict | dict: The success metrics. |

<br />
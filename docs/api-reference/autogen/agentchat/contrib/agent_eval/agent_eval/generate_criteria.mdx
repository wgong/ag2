---
sidebarTitle: generate_criteria
title: autogen.agentchat.contrib.agent_eval.agent_eval.generate_criteria
---

<code class="doc-symbol doc-symbol-heading doc-symbol-function"></code>
#### generate_criteria

```python
generate_criteria(
    llm_config: dict[str, Any] | Literal[False] | None = None,
    task: Task = None,
    additional_instructions: str = '',
    max_round=2,
    use_subcritic: bool = False
) -> 
```

    Creates a list of criteria for evaluating the utility of a given task.<br/>

<b>Parameters:</b>
| Name | Description |
|--|--|
| `llm_config` | llm inference configuration.<br/><br/>**Type:** dict[str, typing.Any] \| Literal[False] \| None<br/><br/>**Default:** None |
| `task` | The task to evaluate.<br/><br/>**Type:** [Task](/docs/api-reference/autogen/agentchat/contrib/agent_eval/task/Task)<br/><br/>**Default:** None |
| `additional_instructions` | Additional instructions for the criteria agent.<br/><br/>**Type:** str<br/><br/>**Default:** '' |
| `max_round=2` |  |
| `use_subcritic` | Whether to use the subcritic agent to generate subcriteria.<br/><br/>**Type:** bool<br/><br/>**Default:** False |

<br />
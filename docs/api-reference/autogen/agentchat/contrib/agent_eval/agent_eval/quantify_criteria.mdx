---
sidebarTitle: quantify_criteria
title: autogen.agentchat.contrib.agent_eval.agent_eval.quantify_criteria
---

<code class="doc-symbol doc-symbol-heading doc-symbol-function"></code>
#### quantify_criteria

```python
quantify_criteria(
    llm_config: dict[str, Any] | Literal[False] | None = None,
    criteria: list[Criterion] = None,
    task: Task = None,
    test_case: str = '',
    ground_truth: str = ''
) -> 
```

    Quantifies the performance of a system using the provided criteria.<br/>

<b>Parameters:</b>
| Name | Description |
|--|--|
| `llm_config` | llm inference configuration.<br/><br/>**Type:** dict[str, typing.Any] \| Literal[False] \| None<br/><br/>**Default:** None |
| `criteria` | A list of criteria for evaluating the utility of a given task.<br/><br/>**Type:** list[[Criterion](/docs/api-reference/autogen/agentchat/contrib/agent_eval/criterion/Criterion)]<br/><br/>**Default:** None |
| `task` | The task to evaluate.<br/><br/>**Type:** [Task](/docs/api-reference/autogen/agentchat/contrib/agent_eval/task/Task)<br/><br/>**Default:** None |
| `test_case` | The test case to evaluate.<br/><br/>**Type:** str<br/><br/>**Default:** '' |
| `ground_truth` | The ground truth for the test case.<br/><br/>**Type:** str<br/><br/>**Default:** '' |

<br />
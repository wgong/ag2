---
sidebarTitle: VectorChromaQueryEngine
title: autogen.agents.experimental.document_agent.VectorChromaQueryEngine
---
<h2 id="autogen.agents.experimental.document_agent.VectorChromaQueryEngine" class="doc doc-heading">
    <code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>
    <span class="doc doc-object-name doc-class-name">VectorChromaQueryEngine</span>
</h2>

```python
VectorChromaQueryEngine(
    db_path: str | None = None,
    embedding_function: Optional[EmbeddingFunction[Any]] = None,
    metadata: dict[str, Any] | None = None,
    llm: ForwardRef('LLM') | None = None,
    collection_name: str | None = None
)
```

    This engine leverages Chromadb to persist document embeddings in a named collection
    and LlamaIndex's VectorStoreIndex to efficiently index and retrieve documents, and generate an answer in response
    to natural language queries. The Chromadb collection serves as the storage layer, while
    the collection name uniquely identifies the set of documents within the persistent database.<br/>This is a autogen.agentchat.contrib.rag.RAGQueryEngine.<br/>Initializes the VectorChromaQueryEngine with db_path, metadata, and embedding function and llm.<br/>

<b>Parameters:</b>
| Name | Description |
|--|--|
| `db_path` | **Type:** str \| None<br/><br/>**Default:** None |
| `embedding_function` | **Type:** Optional[EmbeddingFunction[Any]]<br/><br/>**Default:** None |
| `metadata` | **Type:** dict[str, typing.Any] \| None<br/><br/>**Default:** None |
| `llm` | **Type:** ForwardRef('LLM') \| None<br/><br/>**Default:** None |
| `collection_name` | **Type:** str \| None<br/><br/>**Default:** None |

### Instance Methods

<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>
#### add_docs

```python
add_docs(
    self,
    new_doc_dir: Path | str | None = None,
    new_doc_paths: list[Path | str] | None = None
) -> None
```

    Add additional documents to the existing vector index.<br/>Loads new Docling-parsed Markdown files from a specified directory or a list of file paths
    and inserts them into the current index for future queries.<br/>

<b>Parameters:</b>
| Name | Description |
|--|--|
| `new_doc_dir` | The directory path from which to load additional documents.<br/><br/>If provided, all eligible files in this directory are loaded.<br/><br/>**Type:** pathlib.Path \| str \| None<br/><br/>**Default:** None |
| `new_doc_paths` | A list of file paths specifying additional documents to load.<br/><br/>Each file should be a Docling-parsed Markdown file.<br/><br/>**Type:** list[pathlib.Path \| str] \| None<br/><br/>**Default:** None |

<br />

<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>
#### connect_db

```python
connect_db(self) -> None
```

    Establish a connection to the Chromadb database and initialize the collection.

<br />

<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>
#### get_collection_name

```python
get_collection_name(self) -> str
```

    Get the name of the collection used by the query engine.<br/>Returns: <br/>    The name of the collection.

<b>Returns:</b>
| Type | Description |
|--|--|
| str | The name of the collection. |

<br />

<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>
#### query

```python
query(self, question: str) -> str
```

    Retrieve information from indexed documents by processing a natural language query.<br/>

<b>Parameters:</b>
| Name | Description |
|--|--|
| `question` | A natural language query string used to search the indexed documents.<br/><br/>**Type:** str |

<b>Returns:</b>
| Type | Description |
|--|--|
| str | A string containing the response generated by LLM. |

<br />

<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>
#### validate_query_index

```python
validate_query_index(self) -> None
```

    Ensures an index exists

<br />
---
custom_edit_url: https://github.com/ag2ai/ag2/edit/main/notebook/agentchat_two_users.ipynb
description: Involve multiple human users via function calls and nested chat.
source_notebook: /notebook/agentchat_two_users.ipynb
tags:
- human
- tool/function
title: 'Auto Generated Agent Chat: Collaborative Task Solving with Multiple Agents
  and Human Users'
---

<a href="https://colab.research.google.com/github/ag2ai/ag2/blob/main/notebook/agentchat_two_users.ipynb" class="colab-badge" target="_blank"><img noZoom src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a>
<a href="https://github.com/ag2ai/ag2/blob/main/notebook/agentchat_two_users.ipynb" class="github-badge" target="_blank"><img noZoom src="https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github" alt="Open on GitHub" /></a>




AG2 offers conversable agents powered by LLM, tool, or human, which can
be used to perform tasks collectively via automated chat. This framework
allows tool use and human participation through multi-agent
conversation. Please find documentation about this feature
[here](https://docs.ag2.ai/docs/Use-Cases/agent_chat).

In this notebook, we demonstrate an application involving multiple
agents and human users to work together and accomplish a task.
`AssistantAgent` is an LLM-based agent that can write Python code (in a
Python coding block) for a user to execute for a given task.
`UserProxyAgent` is an agent which serves as a proxy for a user to
execute the code written by `AssistantAgent`. We create multiple
`UserProxyAgent` instances that can represent different human users.

## Requirements

AG2 requires `Python>=3.9`. To run this notebook example, please
install:

```bash
pip install autogen
```


```python
# %pip install "autogen"
```

## Set your API Endpoint

The
[`config_list_from_json`](https://docs.ag2.ai/docs/api-reference/autogen/config_list_from_json#config-list-from-json)
function loads a list of configurations from an environment variable or
a json file.

It first looks for an environment variable of a specified name
(“OAI_CONFIG_LIST” in this example), which needs to be a valid json
string. If that variable is not found, it looks for a json file with the
same name. It filters the configs by models (you can filter by other
keys as well).

The json looks like the following:

```json
[
    {
        "model": "gpt-4",
        "api_key": "<your OpenAI API key here>"
    },
    {
        "model": "gpt-4",
        "api_key": "<your Azure OpenAI API key here>",
        "base_url": "<your Azure OpenAI API base here>",
        "api_type": "azure",
        "api_version": "2024-02-01"
    },
    {
        "model": "gpt-4-32k",
        "api_key": "<your Azure OpenAI API key here>",
        "base_url": "<your Azure OpenAI API base here>",
        "api_type": "azure",
        "api_version": "2024-02-01"
    }
]
```

You can set the value of config_list in any way you prefer. Please refer
to this [User
Guide](https://docs.ag2.ai/docs/user-guide/advanced-concepts/llm-configuration-deep-dive)
for full code examples of the different methods.

```python
import autogen

config_list = autogen.config_list_from_json(
    "OAI_CONFIG_LIST",
    filter_dict={
        "model": ["gpt-4", "gpt4", "gpt-4-32k", "gpt-4-32k-0314", "gpt-4-32k-v0314"],
    },
)
```

## Construct Agents

We define `ask_expert` function to start a conversation between two
agents and return a summary of the result. We construct an assistant
agent named “assistant_for_expert” and a user proxy agent named
“expert”. We specify `human_input_mode` as “ALWAYS” in the user proxy
agent, which will always ask for feedback from the expert user.

```python
def ask_expert(message):
    assistant_for_expert = autogen.AssistantAgent(
        name="assistant_for_expert",
        llm_config={
            "temperature": 0,
            "config_list": config_list,
        },
    )
    expert = autogen.UserProxyAgent(
        name="expert",
        human_input_mode="ALWAYS",
        code_execution_config={
            "work_dir": "expert",
            "use_docker": False,
        },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.
    )

    expert.initiate_chat(assistant_for_expert, message=message)
    expert.stop_reply_at_receive(assistant_for_expert)
    # expert.human_input_mode, expert.max_consecutive_auto_reply = "NEVER", 0
    # final message sent from the expert
    expert.send("summarize the solution and explain the answer in an easy-to-understand way", assistant_for_expert)
    # return the last message the expert received
    return expert.last_message()["content"]
```

We construct another assistant agent named “assistant_for_student” and a
user proxy agent named “student”. We specify `human_input_mode` as
“TERMINATE” in the user proxy agent, which will ask for feedback when it
receives a “TERMINATE” signal from the assistant agent. We set the
`functions` in `AssistantAgent` and `function_map` in `UserProxyAgent`
to use the created `ask_expert` function.

For simplicity, the `ask_expert` function is defined to run locally. For
real applications, the function should run remotely to interact with an
expert user.

```python
assistant_for_student = autogen.AssistantAgent(
    name="assistant_for_student",
    system_message="You are a helpful assistant. Reply TERMINATE when the task is done.",
    llm_config={
        "timeout": 600,
        "cache_seed": 42,
        "config_list": config_list,
        "temperature": 0,
        "functions": [
            {
                "name": "ask_expert",
                "description": "ask expert when you can't solve the problem satisfactorily.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "message": {
                            "type": "string",
                            "description": "question to ask expert. Ensure the question includes enough context, such as the code and the execution result. The expert does not know the conversation between you and the user unless you share the conversation with the expert.",
                        },
                    },
                    "required": ["message"],
                },
            }
        ],
    },
)

student = autogen.UserProxyAgent(
    name="student",
    human_input_mode="TERMINATE",
    max_consecutive_auto_reply=10,
    code_execution_config={
        "work_dir": "student",
        "use_docker": False,
    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.
    function_map={"ask_expert": ask_expert},
)
```

## Perform a task

We invoke the `initiate_chat()` method of the student proxy agent to
start the conversation. When you run the cell below, you will be
prompted to provide feedback after the assistant agent sends a
“TERMINATE” signal at the end of the message. The conversation will
finish if you don’t provide any feedback (by pressing Enter directly).
Before the “TERMINATE” signal, the student proxy agent will try to
execute the code suggested by the assistant agent on behalf of the user.

```python
# the assistant receives a message from the student, which contains the task description
student.initiate_chat(
    assistant_for_student,
    message="""Find $a + b + c$, given that $x+y \\neq -1$ and
\\begin{align}
    ax + by + c & = x + 7,\\
    a + bx + cy & = 2x + 6y,\\
    ay + b + cx & = 4x + y.
\\end{align}.
""",
)
```

When the assistant needs to consult the expert, it suggests a function
call to `ask_expert`. When this happens, a line like the following will
be displayed:

\*\*\*\*\* Suggested function Call: ask_expert \*\*\*\*\*

<div className="edit-url-container">
    <a className="edit-url" href="https://github.com/ag2ai/ag2/edit/main/notebook/agentchat_two_users.ipynb" target='_blank'><Icon icon="pen" iconType="solid" size="13px"/> Edit this page</a>
</div>
